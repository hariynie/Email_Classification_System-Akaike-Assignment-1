{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7b50fe-6727-4a87-958c-9ac7fbad718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "modelsinfo = tf.keras.models.load_model('finalmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c80c23-efa5-4e83-8cb3-1027dec9d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D,LSTM, GlobalMaxPooling1D, Embedding,Bidirectional\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594c135a-4ec9-4dcf-8356-d7256810cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\SUFI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "nltk.download('punkt_tab')\n",
    "import emoji\n",
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text)\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang == \"ta\":  # Tamil language code\n",
    "            translated_text = translator.translate(text, src=\"ta\", dest=\"en\").text\n",
    "            return translated_text\n",
    "        else:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        return text\n",
    "        \n",
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]\n",
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "with open('tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "EMBEDDING_DIM = 300\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    " \n",
    "import re\n",
    "label_names = ['Change', 'Incident', 'Problem', 'Request']\n",
    "def predictsentiment(inputtext):\n",
    "   \n",
    "    cleantext = [remove_punct(inputtext)]\n",
    "    \n",
    "    tokens = [word_tokenize(sen) for sen in cleantext] \n",
    "    lower_tokens = [lower_token(token) for token in tokens] \n",
    "    from nltk.corpus import stopwords\n",
    "    stoplist = stopwords.words('english')\n",
    "    filtered_words = [remove_stop_words(sen) for sen in lower_tokens]\n",
    "    result = [' '.join(sen) for sen in filtered_words] \n",
    "    test_sequence = tokenizer.texts_to_sequences(result)\n",
    "    test_data = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    Y_pred = modelsinfo.predict(test_data)\n",
    "    Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
    "    \n",
    "    print(label_names[Y_pred_classes[0]])\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae58f15-fcbf-4236-aedb-3f4555a55ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Request\n"
     ]
    }
   ],
   "source": [
    "inputinfo=\"hi hello how are you \"\n",
    "\n",
    "predictsentiment(inputinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c2578-540f-4e22-b550-fdfa40690385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
